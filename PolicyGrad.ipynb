{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "\n",
    "# Function to select action\n",
    "def select_action(policy_net, state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy_net(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "# Function to compute discounted rewards\n",
    "def compute_discounted_rewards(rewards, gamma=0.99):\n",
    "    discounted_rewards = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        discounted_rewards.insert(0, R)\n",
    "    return discounted_rewards\n",
    "\n",
    "# Main training loop\n",
    "def train_policy_gradient(env, policy_net, optimizer, num_episodes=1000):\n",
    "    gamma = 0.99\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # Handle tuple returned by env.reset()\n",
    "        \n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        total_reward = 0\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, log_prob = select_action(policy_net, state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            if isinstance(next_state, tuple):\n",
    "                next_state = next_state[0]  # Handle tuple returned by env.step()\n",
    "            \n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-9)\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, reward in zip(log_probs, discounted_rewards):\n",
    "            policy_loss.append(-log_prob * reward)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}, Total Loss: {policy_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000, Total Reward: 30.0, Total Loss: 0.2857\n",
      "Episode 2/1000, Total Reward: 41.0, Total Loss: -0.2600\n",
      "Episode 3/1000, Total Reward: 25.0, Total Loss: 1.1561\n",
      "Episode 4/1000, Total Reward: 19.0, Total Loss: 0.4861\n",
      "Episode 5/1000, Total Reward: 14.0, Total Loss: 0.5826\n",
      "Episode 6/1000, Total Reward: 33.0, Total Loss: -1.1324\n",
      "Episode 7/1000, Total Reward: 15.0, Total Loss: -0.5757\n",
      "Episode 8/1000, Total Reward: 16.0, Total Loss: 0.8058\n",
      "Episode 9/1000, Total Reward: 12.0, Total Loss: 1.9251\n",
      "Episode 10/1000, Total Reward: 10.0, Total Loss: -0.4776\n",
      "Episode 11/1000, Total Reward: 18.0, Total Loss: 3.1433\n",
      "Episode 12/1000, Total Reward: 37.0, Total Loss: 7.8965\n",
      "Episode 13/1000, Total Reward: 30.0, Total Loss: 8.0176\n",
      "Episode 14/1000, Total Reward: 52.0, Total Loss: 0.6334\n",
      "Episode 15/1000, Total Reward: 45.0, Total Loss: -0.4117\n",
      "Episode 16/1000, Total Reward: 43.0, Total Loss: 8.2262\n",
      "Episode 17/1000, Total Reward: 67.0, Total Loss: 0.1068\n",
      "Episode 18/1000, Total Reward: 13.0, Total Loss: 2.9066\n",
      "Episode 19/1000, Total Reward: 27.0, Total Loss: 0.4890\n",
      "Episode 20/1000, Total Reward: 29.0, Total Loss: -0.7881\n",
      "Episode 21/1000, Total Reward: 19.0, Total Loss: -0.8325\n",
      "Episode 22/1000, Total Reward: 35.0, Total Loss: -1.1065\n",
      "Episode 23/1000, Total Reward: 27.0, Total Loss: 0.2854\n",
      "Episode 24/1000, Total Reward: 56.0, Total Loss: -1.2222\n",
      "Episode 25/1000, Total Reward: 50.0, Total Loss: 0.9333\n",
      "Episode 26/1000, Total Reward: 39.0, Total Loss: 0.6407\n",
      "Episode 27/1000, Total Reward: 90.0, Total Loss: 0.5369\n",
      "Episode 28/1000, Total Reward: 23.0, Total Loss: 0.6039\n",
      "Episode 29/1000, Total Reward: 70.0, Total Loss: 0.3685\n",
      "Episode 30/1000, Total Reward: 24.0, Total Loss: 0.6125\n",
      "Episode 31/1000, Total Reward: 104.0, Total Loss: 5.4136\n",
      "Episode 32/1000, Total Reward: 45.0, Total Loss: -1.1156\n",
      "Episode 33/1000, Total Reward: 83.0, Total Loss: 0.8485\n",
      "Episode 34/1000, Total Reward: 35.0, Total Loss: 0.6691\n",
      "Episode 35/1000, Total Reward: 52.0, Total Loss: -2.2097\n",
      "Episode 36/1000, Total Reward: 128.0, Total Loss: 2.4634\n",
      "Episode 37/1000, Total Reward: 188.0, Total Loss: 2.6448\n",
      "Episode 38/1000, Total Reward: 32.0, Total Loss: -0.1980\n",
      "Episode 39/1000, Total Reward: 36.0, Total Loss: 2.0196\n",
      "Episode 40/1000, Total Reward: 73.0, Total Loss: 0.0237\n",
      "Episode 41/1000, Total Reward: 96.0, Total Loss: -4.7801\n",
      "Episode 42/1000, Total Reward: 127.0, Total Loss: 0.4753\n",
      "Episode 43/1000, Total Reward: 166.0, Total Loss: -3.5342\n",
      "Episode 44/1000, Total Reward: 39.0, Total Loss: -6.1929\n",
      "Episode 45/1000, Total Reward: 131.0, Total Loss: -7.2274\n",
      "Episode 46/1000, Total Reward: 90.0, Total Loss: 3.0595\n",
      "Episode 47/1000, Total Reward: 27.0, Total Loss: 2.0223\n",
      "Episode 48/1000, Total Reward: 126.0, Total Loss: -2.5483\n",
      "Episode 49/1000, Total Reward: 30.0, Total Loss: 0.2443\n",
      "Episode 50/1000, Total Reward: 182.0, Total Loss: -8.5100\n",
      "Episode 51/1000, Total Reward: 98.0, Total Loss: -0.1736\n",
      "Episode 52/1000, Total Reward: 124.0, Total Loss: -4.4439\n",
      "Episode 53/1000, Total Reward: 81.0, Total Loss: -0.1888\n",
      "Episode 54/1000, Total Reward: 76.0, Total Loss: 2.2867\n",
      "Episode 55/1000, Total Reward: 87.0, Total Loss: 0.7227\n",
      "Episode 56/1000, Total Reward: 165.0, Total Loss: 1.5940\n",
      "Episode 57/1000, Total Reward: 431.0, Total Loss: 4.9632\n",
      "Episode 58/1000, Total Reward: 73.0, Total Loss: -3.0443\n",
      "Episode 59/1000, Total Reward: 78.0, Total Loss: -0.8765\n",
      "Episode 60/1000, Total Reward: 180.0, Total Loss: -9.1551\n",
      "Episode 61/1000, Total Reward: 70.0, Total Loss: -6.1461\n",
      "Episode 62/1000, Total Reward: 27.0, Total Loss: -0.9993\n",
      "Episode 63/1000, Total Reward: 102.0, Total Loss: -9.5905\n",
      "Episode 64/1000, Total Reward: 150.0, Total Loss: 0.2049\n",
      "Episode 65/1000, Total Reward: 186.0, Total Loss: -2.6329\n",
      "Episode 66/1000, Total Reward: 163.0, Total Loss: 2.0793\n",
      "Episode 67/1000, Total Reward: 173.0, Total Loss: -7.4239\n",
      "Episode 68/1000, Total Reward: 139.0, Total Loss: 3.5131\n",
      "Episode 69/1000, Total Reward: 109.0, Total Loss: -3.7257\n",
      "Episode 70/1000, Total Reward: 108.0, Total Loss: -0.6782\n",
      "Episode 71/1000, Total Reward: 178.0, Total Loss: -11.5728\n",
      "Episode 72/1000, Total Reward: 139.0, Total Loss: 0.5461\n",
      "Episode 73/1000, Total Reward: 104.0, Total Loss: -5.0167\n",
      "Episode 74/1000, Total Reward: 235.0, Total Loss: -6.7024\n",
      "Episode 75/1000, Total Reward: 101.0, Total Loss: -8.5704\n",
      "Episode 76/1000, Total Reward: 151.0, Total Loss: 5.7096\n",
      "Episode 77/1000, Total Reward: 102.0, Total Loss: -2.7617\n",
      "Episode 78/1000, Total Reward: 138.0, Total Loss: 7.7025\n",
      "Episode 79/1000, Total Reward: 166.0, Total Loss: -3.2392\n",
      "Episode 80/1000, Total Reward: 137.0, Total Loss: -3.6985\n",
      "Episode 81/1000, Total Reward: 112.0, Total Loss: -3.1703\n",
      "Episode 82/1000, Total Reward: 92.0, Total Loss: 0.6761\n",
      "Episode 83/1000, Total Reward: 132.0, Total Loss: 4.4637\n",
      "Episode 84/1000, Total Reward: 137.0, Total Loss: -11.4029\n",
      "Episode 85/1000, Total Reward: 127.0, Total Loss: -2.9890\n",
      "Episode 86/1000, Total Reward: 126.0, Total Loss: -0.8192\n",
      "Episode 87/1000, Total Reward: 157.0, Total Loss: -2.2174\n",
      "Episode 88/1000, Total Reward: 161.0, Total Loss: 0.7609\n",
      "Episode 89/1000, Total Reward: 159.0, Total Loss: -0.4095\n",
      "Episode 90/1000, Total Reward: 132.0, Total Loss: -0.9102\n",
      "Episode 91/1000, Total Reward: 159.0, Total Loss: -3.6760\n",
      "Episode 92/1000, Total Reward: 160.0, Total Loss: -5.0210\n",
      "Episode 93/1000, Total Reward: 168.0, Total Loss: 0.2055\n",
      "Episode 94/1000, Total Reward: 152.0, Total Loss: -5.6049\n",
      "Episode 95/1000, Total Reward: 159.0, Total Loss: -2.3719\n",
      "Episode 96/1000, Total Reward: 130.0, Total Loss: -5.6508\n",
      "Episode 97/1000, Total Reward: 144.0, Total Loss: 2.9588\n",
      "Episode 98/1000, Total Reward: 136.0, Total Loss: 0.4912\n",
      "Episode 99/1000, Total Reward: 141.0, Total Loss: -0.4601\n",
      "Episode 100/1000, Total Reward: 130.0, Total Loss: -4.7354\n",
      "Episode 101/1000, Total Reward: 114.0, Total Loss: -8.5270\n",
      "Episode 102/1000, Total Reward: 144.0, Total Loss: -2.1009\n",
      "Episode 103/1000, Total Reward: 138.0, Total Loss: -7.5668\n",
      "Episode 104/1000, Total Reward: 142.0, Total Loss: -2.1708\n",
      "Episode 105/1000, Total Reward: 151.0, Total Loss: 7.3117\n",
      "Episode 106/1000, Total Reward: 164.0, Total Loss: -0.6902\n",
      "Episode 107/1000, Total Reward: 146.0, Total Loss: -4.7793\n",
      "Episode 108/1000, Total Reward: 136.0, Total Loss: -0.4446\n",
      "Episode 109/1000, Total Reward: 155.0, Total Loss: 0.6840\n",
      "Episode 110/1000, Total Reward: 120.0, Total Loss: 4.1121\n",
      "Episode 111/1000, Total Reward: 134.0, Total Loss: -2.3024\n",
      "Episode 112/1000, Total Reward: 137.0, Total Loss: -0.5252\n",
      "Episode 113/1000, Total Reward: 121.0, Total Loss: 0.1412\n",
      "Episode 114/1000, Total Reward: 123.0, Total Loss: -2.9317\n",
      "Episode 115/1000, Total Reward: 124.0, Total Loss: 2.4768\n",
      "Episode 116/1000, Total Reward: 136.0, Total Loss: -7.6137\n",
      "Episode 117/1000, Total Reward: 116.0, Total Loss: 2.5816\n",
      "Episode 118/1000, Total Reward: 122.0, Total Loss: -5.3965\n",
      "Episode 119/1000, Total Reward: 115.0, Total Loss: -8.1535\n",
      "Episode 120/1000, Total Reward: 134.0, Total Loss: -4.4138\n",
      "Episode 121/1000, Total Reward: 129.0, Total Loss: -2.8660\n",
      "Episode 122/1000, Total Reward: 118.0, Total Loss: -1.8081\n",
      "Episode 123/1000, Total Reward: 125.0, Total Loss: 1.9492\n",
      "Episode 124/1000, Total Reward: 111.0, Total Loss: 1.0745\n",
      "Episode 125/1000, Total Reward: 107.0, Total Loss: -6.2762\n",
      "Episode 126/1000, Total Reward: 110.0, Total Loss: -2.9267\n",
      "Episode 127/1000, Total Reward: 108.0, Total Loss: 0.8704\n",
      "Episode 128/1000, Total Reward: 87.0, Total Loss: -1.0968\n",
      "Episode 129/1000, Total Reward: 81.0, Total Loss: -3.2559\n",
      "Episode 130/1000, Total Reward: 27.0, Total Loss: 0.3027\n",
      "Episode 131/1000, Total Reward: 41.0, Total Loss: -0.1282\n",
      "Episode 132/1000, Total Reward: 27.0, Total Loss: -3.1633\n",
      "Episode 133/1000, Total Reward: 28.0, Total Loss: -1.6548\n",
      "Episode 134/1000, Total Reward: 28.0, Total Loss: -2.2346\n",
      "Episode 135/1000, Total Reward: 30.0, Total Loss: 1.0885\n",
      "Episode 136/1000, Total Reward: 27.0, Total Loss: -1.6752\n",
      "Episode 137/1000, Total Reward: 21.0, Total Loss: -7.2029\n",
      "Episode 138/1000, Total Reward: 36.0, Total Loss: -0.4146\n",
      "Episode 139/1000, Total Reward: 23.0, Total Loss: -1.2204\n",
      "Episode 140/1000, Total Reward: 44.0, Total Loss: -2.4531\n",
      "Episode 141/1000, Total Reward: 33.0, Total Loss: 0.9285\n",
      "Episode 142/1000, Total Reward: 38.0, Total Loss: -1.8307\n",
      "Episode 143/1000, Total Reward: 30.0, Total Loss: -0.4463\n",
      "Episode 144/1000, Total Reward: 25.0, Total Loss: -2.4944\n",
      "Episode 145/1000, Total Reward: 32.0, Total Loss: 1.9833\n",
      "Episode 146/1000, Total Reward: 30.0, Total Loss: -0.7568\n",
      "Episode 147/1000, Total Reward: 29.0, Total Loss: 0.1131\n",
      "Episode 148/1000, Total Reward: 34.0, Total Loss: 1.4624\n",
      "Episode 149/1000, Total Reward: 28.0, Total Loss: 3.3374\n",
      "Episode 150/1000, Total Reward: 32.0, Total Loss: -1.2912\n",
      "Episode 151/1000, Total Reward: 35.0, Total Loss: -2.1695\n",
      "Episode 152/1000, Total Reward: 42.0, Total Loss: 2.7048\n",
      "Episode 153/1000, Total Reward: 43.0, Total Loss: -2.7403\n",
      "Episode 154/1000, Total Reward: 41.0, Total Loss: -1.8008\n",
      "Episode 155/1000, Total Reward: 36.0, Total Loss: -0.0629\n",
      "Episode 156/1000, Total Reward: 39.0, Total Loss: -0.0037\n",
      "Episode 157/1000, Total Reward: 67.0, Total Loss: -3.1306\n",
      "Episode 158/1000, Total Reward: 47.0, Total Loss: -4.3466\n",
      "Episode 159/1000, Total Reward: 62.0, Total Loss: -0.2882\n",
      "Episode 160/1000, Total Reward: 51.0, Total Loss: -3.7195\n",
      "Episode 161/1000, Total Reward: 77.0, Total Loss: -3.7150\n",
      "Episode 162/1000, Total Reward: 68.0, Total Loss: -3.4486\n",
      "Episode 163/1000, Total Reward: 81.0, Total Loss: 4.9010\n",
      "Episode 164/1000, Total Reward: 127.0, Total Loss: 3.4146\n",
      "Episode 165/1000, Total Reward: 111.0, Total Loss: -7.9690\n",
      "Episode 166/1000, Total Reward: 121.0, Total Loss: 0.7123\n",
      "Episode 167/1000, Total Reward: 149.0, Total Loss: -1.4752\n",
      "Episode 168/1000, Total Reward: 162.0, Total Loss: -5.1318\n",
      "Episode 169/1000, Total Reward: 160.0, Total Loss: 5.2989\n",
      "Episode 170/1000, Total Reward: 171.0, Total Loss: 1.7916\n",
      "Episode 171/1000, Total Reward: 187.0, Total Loss: -13.8575\n",
      "Episode 172/1000, Total Reward: 223.0, Total Loss: -6.4675\n",
      "Episode 173/1000, Total Reward: 144.0, Total Loss: 2.4382\n",
      "Episode 174/1000, Total Reward: 146.0, Total Loss: -3.4942\n",
      "Episode 175/1000, Total Reward: 448.0, Total Loss: -2.4027\n",
      "Episode 176/1000, Total Reward: 136.0, Total Loss: -2.8035\n",
      "Episode 177/1000, Total Reward: 152.0, Total Loss: 1.7905\n",
      "Episode 178/1000, Total Reward: 255.0, Total Loss: -6.5627\n",
      "Episode 179/1000, Total Reward: 243.0, Total Loss: -6.3387\n",
      "Episode 180/1000, Total Reward: 280.0, Total Loss: -11.8562\n",
      "Episode 181/1000, Total Reward: 190.0, Total Loss: 1.0078\n",
      "Episode 182/1000, Total Reward: 258.0, Total Loss: -8.7455\n",
      "Episode 183/1000, Total Reward: 341.0, Total Loss: -7.4094\n",
      "Episode 184/1000, Total Reward: 307.0, Total Loss: -8.0379\n",
      "Episode 185/1000, Total Reward: 322.0, Total Loss: -10.3734\n",
      "Episode 186/1000, Total Reward: 500.0, Total Loss: -12.1469\n",
      "Episode 187/1000, Total Reward: 293.0, Total Loss: -7.7421\n",
      "Episode 188/1000, Total Reward: 416.0, Total Loss: -17.3822\n",
      "Episode 189/1000, Total Reward: 399.0, Total Loss: -8.0181\n",
      "Episode 190/1000, Total Reward: 333.0, Total Loss: -4.2206\n",
      "Episode 191/1000, Total Reward: 314.0, Total Loss: -1.9277\n",
      "Episode 192/1000, Total Reward: 345.0, Total Loss: -11.9858\n",
      "Episode 193/1000, Total Reward: 405.0, Total Loss: -13.2510\n",
      "Episode 194/1000, Total Reward: 289.0, Total Loss: -13.2307\n",
      "Episode 195/1000, Total Reward: 451.0, Total Loss: -17.7544\n",
      "Episode 196/1000, Total Reward: 500.0, Total Loss: 6.2941\n",
      "Episode 197/1000, Total Reward: 378.0, Total Loss: -11.2588\n",
      "Episode 198/1000, Total Reward: 323.0, Total Loss: 6.8629\n",
      "Episode 199/1000, Total Reward: 500.0, Total Loss: -4.5634\n",
      "Episode 200/1000, Total Reward: 438.0, Total Loss: -15.2532\n",
      "Episode 201/1000, Total Reward: 500.0, Total Loss: -10.9481\n",
      "Episode 202/1000, Total Reward: 331.0, Total Loss: -4.3889\n",
      "Episode 203/1000, Total Reward: 342.0, Total Loss: 3.2637\n",
      "Episode 204/1000, Total Reward: 319.0, Total Loss: 13.4168\n",
      "Episode 205/1000, Total Reward: 421.0, Total Loss: 8.2097\n",
      "Episode 206/1000, Total Reward: 450.0, Total Loss: -21.9257\n",
      "Episode 207/1000, Total Reward: 470.0, Total Loss: 9.0212\n",
      "Episode 208/1000, Total Reward: 500.0, Total Loss: -8.7020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(policy_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Train the policy gradient agent\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtrain_policy_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 51\u001b[0m, in \u001b[0;36mtrain_policy_gradient\u001b[1;34m(env, policy_net, optimizer, num_episodes)\u001b[0m\n\u001b[0;32m     49\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 51\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     next_state, reward, done, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_state, \u001b[38;5;28mtuple\u001b[39m):\n",
      "Cell \u001b[1;32mIn[26], line 23\u001b[0m, in \u001b[0;36mselect_action\u001b[1;34m(policy_net, state)\u001b[0m\n\u001b[0;32m     21\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     22\u001b[0m probs \u001b[38;5;241m=\u001b[39m policy_net(state)\n\u001b[1;32m---> 23\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m action \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), m\u001b[38;5;241m.\u001b[39mlog_prob(action)\n",
      "File \u001b[1;32mc:\\Users\\pitiw\\miniconda3\\Lib\\site-packages\\torch\\distributions\\categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     69\u001b[0m )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pitiw\\miniconda3\\Lib\\site-packages\\torch\\distributions\\distribution.py:66\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     65\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 66\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pitiw\\miniconda3\\Lib\\site-packages\\torch\\distributions\\constraints.py:440\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m ((value\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize environment, policy network, and optimizer\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "policy_net = PolicyNetwork(state_size, action_size)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "# Train the policy gradient agent\n",
    "train_policy_gradient(env, policy_net, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
